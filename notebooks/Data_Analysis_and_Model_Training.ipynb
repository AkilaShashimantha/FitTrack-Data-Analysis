{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Project Setup and Data Loading\n",
    "\n",
    "First, we import the necessary libraries for data manipulation, visualization, and machine learning. We then define the paths to our datasets and load them into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Define file paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_path = os.path.join(project_root, 'data')\n",
    "activity_file = os.path.join(data_path, 'dailyActivity_merged.csv')\n",
    "sleep_file = os.path.join(data_path, 'sleepDay_merged.csv')\n",
    "\n",
    "# Load the datasets\n",
    "df_activity = pd.read_csv(activity_file)\n",
    "df_sleep = pd.read_csv(sleep_file)\n",
    "\n",
    "print(\"Activity Data Head:\")\n",
    "display(df_activity.head())\n",
    "print(\"\\nSleep Data Head:\")\n",
    "display(df_sleep.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Cleaning and Merging\n",
    "\n",
    "To analyze activity and sleep together, we need a single dataset. Here, we'll perform the following cleaning steps:\n",
    "1.  **Convert date columns** to a consistent `datetime` format.\n",
    "2.  **Merge** the two DataFrames on `Id` and the date.\n",
    "3.  **Drop** unnecessary columns.\n",
    "4.  **Remove** any duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Activity Data\n",
    "df_activity['ActivityDate'] = pd.to_datetime(df_activity['ActivityDate'], format='%m/%d/%Y')\n",
    "\n",
    "# Clean Sleep Data\n",
    "df_sleep['SleepDay'] = pd.to_datetime(df_sleep['SleepDay'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "df_sleep['ActivityDate'] = df_sleep['SleepDay'].dt.date\n",
    "df_sleep['ActivityDate'] = pd.to_datetime(df_sleep['ActivityDate'])\n",
    "\n",
    "# Merge DataFrames\n",
    "df_merged = pd.merge(df_activity, df_sleep, on=['Id', 'ActivityDate'], how='inner')\n",
    "\n",
    "# Final Cleaning\n",
    "df_merged = df_merged.drop(['TrackerDistance', 'LoggedActivitiesDistance', 'SleepDay'], axis=1)\n",
    "df_merged.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Merged and Cleaned Data Info:\")\n",
    "df_merged.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows of merged data:\")\n",
    "display(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now we explore the cleaned data to find initial insights and relationships. We will visualize:\n",
    "1.  A **correlation heatmap** to see how different variables relate to each other.\n",
    "2.  A **scatter plot** to investigate the relationship between `TotalSteps` and `Calories` burned.\n",
    "3.  A **scatter plot** to see the connection between `VeryActiveMinutes` and `TotalMinutesAsleep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation Heatmap\n",
    "corr = df_merged.corr(numeric_only=True)\n",
    "heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=corr.values,\n",
    "    x=corr.columns,\n",
    "    y=corr.columns,\n",
    "    colorscale='Viridis',\n",
    "    zmin=-1,\n",
    "    zmax=1\n",
    "))\n",
    "heatmap.update_layout(title='Correlation Matrix of Fitness Data')\n",
    "heatmap.show()\n",
    "\n",
    "# 2. Steps vs. Calories\n",
    "scatter_steps_calories = px.scatter(\n",
    "    df_merged, \n",
    "    x='TotalSteps', \n",
    "    y='Calories', \n",
    "    trendline='ols', \n",
    "    color='Calories',\n",
    "    title='Total Steps vs. Calories Burned',\n",
    "    labels={'TotalSteps': 'Total Steps', 'Calories': 'Calories Burned'}\n",
    ")\n",
    "scatter_steps_calories.show()\n",
    "\n",
    "# 3. Sleep vs. Activity\n",
    "scatter_sleep_activity = px.scatter(\n",
    "    df_merged, \n",
    "    x='VeryActiveMinutes', \n",
    "    y='TotalMinutesAsleep', \n",
    "    color='TotalMinutesAsleep',\n",
    "    color_continuous_scale='Cividis_r',\n",
    "    title='Sleep Duration vs. Very Active Minutes',\n",
    "    labels={'VeryActiveMinutes': 'Very Active Minutes', 'TotalMinutesAsleep': 'Minutes Asleep'}\n",
    ")\n",
    "scatter_sleep_activity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Engineering & Preprocessing\n",
    "\n",
    "Before training, we must prepare our data.\n",
    "1.  **Select Features:** Choose the columns (`features`) that will be used to predict the `target`.\n",
    "2.  **Split Data:** Divide the data into training and testing sets.\n",
    "3.  **Scale Features:** Use `StandardScaler` to normalize our features. This is crucial for distance-based algorithms like Ridge and Lasso regression and generally improves model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'TotalSteps', 'TotalDistance', 'VeryActiveMinutes', \n",
    "    'FairlyActiveMinutes', 'LightlyActiveMinutes', 'SedentaryMinutes',\n",
    "    'TotalMinutesAsleep'\n",
    "]\n",
    "target = 'Calories'\n",
    "\n",
    "X = df_merged[features]\n",
    "y = df_merged[target]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Testing data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Training & Evaluation\n",
    "\n",
    "Here's where we train our 5 different regression models. For each model, we will:\n",
    "1.  **Train** it on the scaled training data.\n",
    "2.  **Make predictions** on the scaled test data.\n",
    "3.  **Evaluate** its performance using Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
    "\n",
    "Finally, we'll visualize the R-squared scores to easily compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R-squared': r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize model performance\n",
    "fig = px.bar(\n",
    "    results_df.sort_values('R-squared', ascending=False),\n",
    "    x='Model',\n",
    "    y='R-squared',\n",
    "    color='Model',\n",
    "    title='Comparison of Model Performance (R-squared)',\n",
    "    labels={'R-squared': 'R-squared Score'}\n",
    ")\n",
    "fig.update_layout(yaxis=dict(range=[0,1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Saving the Best Model\n",
    "\n",
    "Based on the R-squared value, we identify the best-performing model. We then save this model and the scaler to disk using `joblib`. These files (`best_model.pkl` and `scaler.pkl`) will be loaded by our Streamlit application to make live predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "best_model_name = results_df.loc[results_df['R-squared'].idxmax()]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Define output paths (in the project root)\n",
    "model_output_path = os.path.join(project_root, 'best_model.pkl')\n",
    "scaler_output_path = os.path.join(project_root, 'scaler.pkl')\n",
    "\n",
    "# Save the model and scaler\n",
    "joblib.dump(best_model, model_output_path)\n",
    "joblib.dump(scaler, scaler_output_path)\n",
    "\n",
    "# Also save the cleaned data for the app\n",
    "cleaned_data_output_path = os.path.join(data_path, 'cleaned_fitness_data.csv')\n",
    "df_merged.to_csv(cleaned_data_output_path, index=False)\n",
    "\n",
    "print(f\"Best model '{best_model_name}' saved to: {model_output_path}\")\n",
    "print(f\"Scaler saved to: {scaler_output_path}\")\n",
    "print(f\"Cleaned data saved to: {cleaned_data_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
